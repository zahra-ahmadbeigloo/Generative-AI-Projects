{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Loading and Text Processing for BERT**"
      ],
      "metadata": {
        "id": "Ord_oSpch3do"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "y6eadeIchzOx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "159a5f14-7cdd-459c-9d96-ba362eff3210"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: portalocker==2.8.2 in /usr/local/lib/python3.11/dist-packages (2.8.2)\n"
          ]
        }
      ],
      "source": [
        "# !pip install numpy==1.26.0\n",
        "# !pip install torch==2.2.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
        "# !pip install torchtext==0.17.2\n",
        "# !pip install torchdata==0.7.1\n",
        "!pip install portalocker==2.8.2\n",
        "# !pip install pandas==2.2.1\n",
        "# !pip install transformers==4.35.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.vocab import Vocab\n",
        "from torch import Tensor\n",
        "from torch.nn import Transformer\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from itertools import chain\n",
        "from itertools import islice\n",
        "from torchtext.datasets import IMDB\n",
        "from copy import deepcopy\n",
        "import random\n",
        "import csv\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# You can also use this section to suppress warnings generated by your code:\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "tp1Dgnmyh8xY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Tokenization and Vocabulary Building**"
      ],
      "metadata": {
        "id": "YunhSghHiAze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for label, data_sample in data_iter:\n",
        "        yield tokenizer(data_sample)\n",
        "\n",
        "PAD_IDX,CLS_IDX, SEP_IDX,  MASK_IDX,UNK_IDX= 0, 1, 2, 3, 4\n",
        "special_symbols = ['[PAD]','[CLS]', '[SEP]','[MASK]','[UNK]']"
      ],
      "metadata": {
        "id": "vMJHb6BqiJj4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter, test_iter = IMDB(split=('train', 'test'))\n",
        "all_data_iter = chain(train_iter, test_iter)\n",
        "\n",
        "vocab=build_vocab_from_iterator(yield_tokens(all_data_iter),specials=special_symbols,special_first=True)\n",
        "vocab.set_default_index(UNK_IDX)\n",
        "VOCAB_SIZE=len(vocab)\n",
        "print(VOCAB_SIZE)"
      ],
      "metadata": {
        "id": "DaTz0dpfiW-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b923461-bdf6-4188-ee52-95baf46ec756"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "147161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_to_index=lambda text: [vocab(token) for token in tokenizer(text)]\n",
        "index_to_en = lambda seq_en: \" \".join([vocab.get_itos()[index] for index in seq_en])"
      ],
      "metadata": {
        "id": "apbhsSq3ics-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Text Masking and Data Preparation for BERT**"
      ],
      "metadata": {
        "id": "KaenJ4PKiJOO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Masking"
      ],
      "metadata": {
        "id": "V5XrkaAjjCqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bernoulli_true_false(p):\n",
        "    bernoulli_dist = torch.distributions.Bernoulli(torch.tensor([p]))  # probability p\n",
        "    return bernoulli_dist.sample().item() == 1  # convert 1 to True and 0 to False"
      ],
      "metadata": {
        "id": "dNpn_2_ljCdX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Masking(token):\n",
        "    mask = bernoulli_true_false(0.2)  # Decide whether to mask this token (20% chance)\n",
        "    # If mask is False, immediately return with '[PAD]' label\n",
        "    if not mask:\n",
        "        return token, '[PAD]'\n",
        "\n",
        "    # If mask is True, proceed with further operations\n",
        "    # Randomly decide on an operation (50% chance each)\n",
        "    random_opp = bernoulli_true_false(0.5)\n",
        "    random_swich = bernoulli_true_false(0.5)\n",
        "\n",
        "    # Case 1: If mask, random_opp, and random_swich are True (Masked)\n",
        "    if mask and random_opp and random_swich:\n",
        "        # Replace the token with '[MASK]' and set label to a random token\n",
        "        mask_label = index_to_en(torch.randint(0, VOCAB_SIZE, (1,)))\n",
        "        token_ = '[MASK]'\n",
        "\n",
        "    # Case 2: If mask and random_opp are True, but random_swich is False (Unchanged)\n",
        "    elif mask and random_opp and not random_swich:\n",
        "        # Leave the token unchanged and set label to the same token\n",
        "        token_ = token\n",
        "        mask_label = token\n",
        "\n",
        "    # Case 3: If mask is True, but random_opp is False (Noise)\n",
        "    else:\n",
        "        # Replace the token with '[MASK]' and set label to the original token\n",
        "        token_ = '[MASK]'\n",
        "        mask_label = token\n",
        "\n",
        "    return token_, mask_label"
      ],
      "metadata": {
        "id": "NHkxRVv6irMG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation for MLM"
      ],
      "metadata": {
        "id": "0GCP7Wksk1og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_for_mlm(tokens, include_raw_tokens=False):\n",
        "    \"\"\"\n",
        "    Prepares tokenized text for BERT's Masked Language Model (MLM) training.\n",
        "    \"\"\"\n",
        "    bert_input = []  # List to store sentences processed for BERT's MLM\n",
        "    bert_label = []  # List to store labels for each token (mask, random, or unchanged)\n",
        "    raw_tokens_list = []  # List to store raw tokens if needed\n",
        "    current_bert_input = []\n",
        "    current_bert_label = []\n",
        "    current_raw_tokens = []\n",
        "\n",
        "    for token in tokens:\n",
        "        # Apply BERT's MLM masking strategy to the token\n",
        "        masked_token, mask_label = Masking(token)\n",
        "\n",
        "        # Append the processed token and its label to the current sentence and label list\n",
        "        current_bert_input.append(masked_token)\n",
        "        current_bert_label.append(mask_label)\n",
        "\n",
        "        # If raw tokens are to be included, append the original token to the current raw tokens list\n",
        "        if include_raw_tokens:\n",
        "            current_raw_tokens.append(token)\n",
        "\n",
        "        # Check if the token is a sentence delimiter (., ?, !)\n",
        "        if token in ['.', '?', '!']:\n",
        "            # If current sentence has more than two tokens, consider it a valid sentence\n",
        "            if len(current_bert_input) > 2:\n",
        "                bert_input.append(current_bert_input)\n",
        "                bert_label.append(current_bert_label)\n",
        "                # If including raw tokens, add the current list of raw tokens to the raw tokens list\n",
        "                if include_raw_tokens:\n",
        "                    raw_tokens_list.append(current_raw_tokens)\n",
        "\n",
        "                # Reset the lists for the next sentence\n",
        "                current_bert_input = []\n",
        "                current_bert_label = []\n",
        "                current_raw_tokens = []\n",
        "            else:\n",
        "                # If the current sentence is too short, discard it and reset lists\n",
        "                current_bert_input = []\n",
        "                current_bert_label = []\n",
        "                current_raw_tokens = []\n",
        "\n",
        "    # Add any remaining tokens as a sentence if there are any\n",
        "    if current_bert_input:\n",
        "        bert_input.append(current_bert_input)\n",
        "        bert_label.append(current_bert_label)\n",
        "        if include_raw_tokens:\n",
        "            raw_tokens_list.append(current_raw_tokens)\n",
        "\n",
        "    # Return the prepared lists for BERT's MLM training\n",
        "    return (bert_input, bert_label, raw_tokens_list) if include_raw_tokens else (bert_input, bert_label)"
      ],
      "metadata": {
        "id": "pMlwKl7bk3lO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation for NSP"
      ],
      "metadata": {
        "id": "8OVuwZmHl6SO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_for_nsp(input_sentences, input_masked_labels):\n",
        "    \"\"\"\n",
        "    Prepares data for Next Sentence Prediction (NSP) task in BERT training.\n",
        "\n",
        "    Args:\n",
        "    input_sentences (list): List of tokenized sentences.\n",
        "    input_masked_labels (list): Corresponding list of masked labels for the sentences.\n",
        "\n",
        "    Returns:\n",
        "    bert_input (list): List of sentence pairs for BERT input.\n",
        "    bert_label (list): List of masked labels for the sentence pairs.\n",
        "    is_next (list): Binary label list where 1 indicates 'next sentence' and 0 indicates 'not next sentence'.\n",
        "    \"\"\"\n",
        "    if len(input_sentences) < 2:\n",
        "       raise ValueError(\"must have two same number of items.\")\n",
        "\n",
        "\n",
        "    # Verify that both input lists are of the same length and have a sufficient number of sentences\n",
        "    if len(input_sentences) != len(input_masked_labels):\n",
        "        raise ValueError(\"Both lists must have the same number of items.\")\n",
        "\n",
        "    bert_input = []\n",
        "    bert_label = []\n",
        "    is_next = []\n",
        "\n",
        "    available_indices = list(range(len(input_sentences)))\n",
        "\n",
        "    while len(available_indices) >= 2:\n",
        "        if random.random() < 0.5:\n",
        "            # Choose two consecutive sentences to simulate the 'next sentence' scenario\n",
        "            index = random.choice(available_indices[:-1])  # Exclude the last index\n",
        "            # append list and add  '[CLS]' and  '[SEP]' tokens\n",
        "            bert_input.append([['[CLS]']+input_sentences[index]+ ['[SEP]'],input_sentences[index + 1]+ ['[SEP]']])\n",
        "            bert_label.append([['[PAD]']+input_masked_labels[index]+['[PAD]'], input_masked_labels[index + 1]+ ['[PAD]']])\n",
        "            is_next.append(1)  # Label 1 indicates these sentences are consecutive\n",
        "\n",
        "            # Remove the used indices\n",
        "            available_indices.remove(index)\n",
        "            if index + 1 in available_indices:\n",
        "                available_indices.remove(index + 1)\n",
        "        else:\n",
        "            # Choose two random distinct sentences to simulate the 'not next sentence' scenario\n",
        "            indices = random.sample(available_indices, 2)\n",
        "            bert_input.append([['[CLS]']+input_sentences[indices[0]]+['[SEP]'],input_sentences[indices[1]]+ ['[SEP]']])\n",
        "            bert_label.append([['[PAD]']+input_masked_labels[indices[0]]+['[PAD]'], input_masked_labels[indices[1]]+['[PAD]']])\n",
        "            is_next.append(0)  # Label 0 indicates these sentences are not consecutive\n",
        "\n",
        "            # Remove the used indices\n",
        "            available_indices.remove(indices[0])\n",
        "            available_indices.remove(indices[1])\n",
        "\n",
        "\n",
        "\n",
        "    return bert_input, bert_label, is_next"
      ],
      "metadata": {
        "id": "doUMmpe1l8XM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finalizing BERT Inputs"
      ],
      "metadata": {
        "id": "5NF6ym2cpAJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_bert_final_inputs(bert_inputs, bert_labels, is_nexts,to_tenor=True):\n",
        "    \"\"\"\n",
        "    Prepare the final input lists for BERT training.\n",
        "    \"\"\"\n",
        "    def zero_pad_list_pair(pair_, pad='[PAD]'):\n",
        "        pair=deepcopy(pair_)\n",
        "        max_len = max(len(pair[0]), len(pair[1]))\n",
        "        #append [PAD] to each sentence in the pair till the maximum length reaches\n",
        "        pair[0].extend([pad] * (max_len - len(pair[0])))\n",
        "        pair[1].extend([pad] * (max_len - len(pair[1])))\n",
        "        return pair[0], pair[1]\n",
        "\n",
        "    #flatten the tensor\n",
        "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "    #transform tokens to vocab indices\n",
        "    tokens_to_index=lambda tokens: [vocab[token] for token in tokens]\n",
        "\n",
        "    bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_final = [], [], [], []\n",
        "\n",
        "    for bert_input, bert_label,is_next in zip(bert_inputs, bert_labels,is_nexts):\n",
        "        # Create segment labels for each pair of sentences\n",
        "        segment_label = [[1] * len(bert_input[0]), [2] * len(bert_input[1])]\n",
        "\n",
        "        # Zero-pad the bert_input and bert_label and segment_label\n",
        "        bert_input_padded = zero_pad_list_pair(bert_input)\n",
        "        bert_label_padded = zero_pad_list_pair(bert_label)\n",
        "        segment_label_padded = zero_pad_list_pair(segment_label,pad=0)\n",
        "\n",
        "        #convert to tensors\n",
        "        if to_tenor:\n",
        "\n",
        "            # Flatten the padded inputs and labels, transform tokens to their corresponding vocab indices, and convert them to tensors\n",
        "            bert_inputs_final.append(torch.tensor(tokens_to_index(flatten(bert_input_padded)),dtype=torch.int64))\n",
        "            #bert_labels_final.append(torch.tensor(tokens_to_index(flatten(bert_label_padded)),dtype=torch.int64))\n",
        "            bert_labels_final.append(torch.tensor(tokens_to_index(flatten(bert_label_padded)),dtype=torch.int64))\n",
        "            segment_labels_final.append(torch.tensor(flatten(segment_label_padded),dtype=torch.int64))\n",
        "            is_nexts_final.append(is_next)\n",
        "\n",
        "        else:\n",
        "          # Flatten the padded inputs and labels\n",
        "            bert_inputs_final.append(flatten(bert_input_padded))\n",
        "            bert_labels_final.append(flatten(bert_label_padded))\n",
        "            segment_labels_final.append(flatten(segment_label_padded))\n",
        "            is_nexts_final.append(is_next)\n",
        "\n",
        "    return bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_final\n"
      ],
      "metadata": {
        "id": "DZ9vrHVYpCGp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Preparing the Dataset**"
      ],
      "metadata": {
        "id": "Y5S8CbBuphVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file_path ='train_bert_data_new.csv'\n",
        "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
        "    csv_writer = csv.writer(file)\n",
        "    csv_writer.writerow(['Original Text', 'BERT Input', 'BERT Label', 'Segment Label', 'Is Next'])\n",
        "\n",
        "    # Wrap train_iter with tqdm for a progress bar\n",
        "    for n, (_, sample) in enumerate(tqdm(train_iter, desc=\"Processing samples\")):\n",
        "        # Tokenize the sample input\n",
        "        tokens = tokenizer(sample)\n",
        "        # Create MLM inputs and labels\n",
        "        bert_input, bert_label = prepare_for_mlm(tokens, include_raw_tokens=False)\n",
        "        if len(bert_input) < 2:\n",
        "            continue\n",
        "        # Create NSP pairs, token labels, and is_next label\n",
        "        bert_inputs, bert_labels, is_nexts = process_for_nsp(bert_input, bert_label)\n",
        "        # add zero-paddings, map tokens to vocab indices and create segment labels\n",
        "        bert_inputs, bert_labels, segment_labels, is_nexts = prepare_bert_final_inputs(bert_inputs, bert_labels, is_nexts)\n",
        "        # convert tensors to lists, convert lists to JSON-formatted strings\n",
        "        for bert_input, bert_label, segment_label, is_next in zip(bert_inputs, bert_labels, segment_labels, is_nexts):\n",
        "            bert_input_str = json.dumps(bert_input.tolist())\n",
        "            bert_label_str = json.dumps(bert_label.tolist())\n",
        "            segment_label_str = ','.join(map(str, segment_label.tolist()))\n",
        "            # Write the data to a CSV file row-by-row\n",
        "            csv_writer.writerow([sample, bert_input_str, bert_label_str, segment_label_str, is_next])"
      ],
      "metadata": {
        "id": "USfUUeJTpnDv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67a926f7-4f3a-4758-d420-0f64801429bb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing samples: 25000it [2:09:07,  3.23it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class BertPretrainDataset(Dataset):\n",
        "    def __init__(self, csv_path):\n",
        "        self.data = pd.read_csv(csv_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        bert_input = torch.tensor(json.loads(self.data.iloc[idx]['BERT Input']), dtype=torch.long)\n",
        "        bert_label = torch.tensor(json.loads(self.data.iloc[idx]['BERT Label']), dtype=torch.long)\n",
        "        segment_label = torch.tensor([int(x) for x in self.data.iloc[idx]['Segment Label'].split(',')], dtype=torch.long)\n",
        "        is_next = torch.tensor(int(self.data.iloc[idx]['Is Next']), dtype=torch.long)\n",
        "\n",
        "        return {'input_ids': bert_input,\n",
        "                'labels': bert_label,\n",
        "                'token_type_ids': segment_label,\n",
        "                'next_sentence_label': is_next}"
      ],
      "metadata": {
        "id": "gKsMJUoTR-7X"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids = [item['input_ids'] for item in batch]\n",
        "    labels = [item['labels'] for item in batch]\n",
        "    token_type_ids = [item['token_type_ids'] for item in batch]\n",
        "    next_sentence_label = torch.stack([item['next_sentence_label'] for item in batch])\n",
        "\n",
        "    # Pad sequences to same length within the batch\n",
        "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
        "    labels = pad_sequence(labels, batch_first=True, padding_value=-100)  # -100 is ignored in loss\n",
        "    token_type_ids = pad_sequence(token_type_ids, batch_first=True, padding_value=0)\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'labels': labels,\n",
        "        'token_type_ids': token_type_ids,\n",
        "        'next_sentence_label': next_sentence_label\n",
        "    }\n"
      ],
      "metadata": {
        "id": "i2izRbml-1tj"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = BertPretrainDataset('train_bert_data_new.csv')\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "skr0JRPOSQQ0"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_batch = next(iter(train_loader))\n",
        "\n",
        "for i in range(2):\n",
        "    print(f\"\\nBatch Sample {i}:\")\n",
        "    for key in first_batch:\n",
        "        print(f\"{key}: {first_batch[key][i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5Ku8eRz92Rd",
        "outputId": "5496b31d-ce20-4f72-a788-d3d5e7e361bd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Batch Sample 0:\n",
            "input_ids: tensor([  1,  23,   3, 123,  10,   3,   3,   2,  98,  12,  30,   3,   2,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0])\n",
            "labels: tensor([    0,     0,     5,     0,     0,   540,     6,     0,     0,     0,\n",
            "            0, 65318,     0,     0,     0,     0,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100])\n",
            "token_type_ids: tensor([1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "next_sentence_label: 1\n",
            "\n",
            "Batch Sample 1:\n",
            "input_ids: tensor([    1,     3,    65,   682,    56,    11,    92,     3,   505,  1107,\n",
            "            8, 21262,     6,     2,    65,    94,     3,    12,    30,  6330,\n",
            "          242, 15552,     3,     2,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0])\n",
            "labels: tensor([    0,    41,     0,     0,     0,     0,     0,    13,     0,     0,\n",
            "            8,     0,     0,     0,     0,     0,   162,     0,     0,     0,\n",
            "            0,     0, 24252,     0,     0,     0,     0,     0,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100])\n",
            "token_type_ids: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "next_sentence_label: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QhBJbITY-B6L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
