{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"},
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Building BERT from Scratch**"
      ],
      "metadata": {
        "id": "kQhtpNUrn3Qm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "TbNYhpw_naMV"
      },
      "outputs": [],
      "source": [
        "!pip install --no-cache-dir numpy==1.26.4 pandas==2.2.1\n",
        "!pip install --no-cache-dir torch==2.3.0 torchtext==0.18.0 transformers==4.35.2\n",
        "!pip install --no-cache-dir matplotlib==3.9.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import Tensor\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "from transformers import BertTokenizer\n",
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torchtext.vocab import Vocab,build_vocab_from_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.datasets import IMDB\n",
        "import random\n",
        "from itertools import chain\n",
        "import pandas as pd\n",
        "from copy import deepcopy\n",
        "import csv\n",
        "import json\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# You can also use this section to suppress warnings generated by your code:\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hl_fOXxhoBOV",
        "outputId": "1a4594b9-e628-47cc-b032-698bd16a8054"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.11/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.11/dist-packages/torchtext/utils.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.11/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.11/dist-packages/torchtext/datasets/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Loading Data**"
      ],
      "metadata": {
        "id": "OUzDtc8DpzY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# Remove any earlier instance of the dataset loaded\n",
        "if [ -d \"bert_dataset\" ]; then\n",
        "    rm -rf bert_dataset\n",
        "fi"
      ],
      "metadata": {
        "id": "V1LDlovrp5Ro"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O BERT_dataset.zip https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/bZaoQD52DcMpE7-kxwAG8A.zip\n",
        "!unzip BERT_dataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qUZWYCCp-bl",
        "outputId": "7d68655c-7d8f-4291-b9ed-c716b0076b30"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-06 15:43:15--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/bZaoQD52DcMpE7-kxwAG8A.zip\n",
            "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 198.23.119.245\n",
            "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|198.23.119.245|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 88958506 (85M) [application/zip]\n",
            "Saving to: ‘BERT_dataset.zip’\n",
            "\n",
            "BERT_dataset.zip    100%[===================>]  84.84M  55.7MB/s    in 1.5s    \n",
            "\n",
            "2025-07-06 15:43:17 (55.7 MB/s) - ‘BERT_dataset.zip’ saved [88958506/88958506]\n",
            "\n",
            "Archive:  BERT_dataset.zip\n",
            "   creating: bert_dataset/\n",
            "  inflating: bert_dataset/.DS_Store  \n",
            "  inflating: bert_dataset/bert_train_data.csv  \n",
            "  inflating: bert_dataset/bert_test_data_sampled.csv  \n",
            "  inflating: bert_dataset/bert_test_data.csv  \n",
            "  inflating: bert_dataset/bert_train_data_sampled.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTCSVDataset(Dataset):\n",
        "    def __init__(self, filename):\n",
        "        self.data = pd.read_csv(filename)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        try:\n",
        "\n",
        "            bert_input = torch.tensor(json.loads(row['BERT Input']), dtype=torch.long)\n",
        "            bert_label = torch.tensor(json.loads(row['BERT Label']), dtype=torch.long)\n",
        "            segment_label = torch.tensor([int(x) for x in row['Segment Label'].split(',')], dtype=torch.long)\n",
        "            is_next = torch.tensor(row['Is Next'], dtype=torch.long)\n",
        "            original_text = row['Original Text']  # If you want to use it\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Error decoding JSON for row {idx}: {e}\")\n",
        "            print(\"BERT Input:\", row['BERT Input'])\n",
        "            print(\"BERT Label:\", row['BERT Label'])\n",
        "            # Handle the error, e.g., by skipping this row or using default values\n",
        "            return None  # or some default values\n",
        "\n",
        "            # Tokenizing the original text with BERT\n",
        "        encoded_input = self.tokenizer.encode_plus(\n",
        "            original_text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=512,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        input_ids = encoded_input['input_ids'].squeeze()\n",
        "        attention_mask = encoded_input['attention_mask'].squeeze()\n",
        "\n",
        "        return(bert_input, bert_label, segment_label, is_next, input_ids, attention_mask, original_text)"
      ],
      "metadata": {
        "id": "wXkaNXdmqDhk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_IDX = 0\n",
        "def collate_batch(batch):\n",
        "\n",
        "\n",
        "    bert_inputs_batch, bert_labels_batch, segment_labels_batch, is_nexts_batch,input_ids_batch,attention_mask_batch,original_text_battch = [], [], [], [],[],[],[]\n",
        "\n",
        "    for bert_input, bert_label, segment_label, is_next,input_ids,attention_mask,original_text in batch:\n",
        "        # Convert each sequence to a tensor and append to the respective list\n",
        "        bert_inputs_batch.append(torch.tensor(bert_input, dtype=torch.long))\n",
        "        bert_labels_batch.append(torch.tensor(bert_label, dtype=torch.long))\n",
        "        segment_labels_batch.append(torch.tensor(segment_label, dtype=torch.long))\n",
        "        is_nexts_batch.append(is_next)\n",
        "        input_ids_batch.append(input_ids)\n",
        "        attention_mask_batch.append(attention_mask)\n",
        "        original_text_battch.append(original_text)\n",
        "\n",
        "    # Pad the sequences in the batch\n",
        "    bert_inputs_final = pad_sequence(bert_inputs_batch, padding_value=PAD_IDX, batch_first=False)\n",
        "    bert_labels_final = pad_sequence(bert_labels_batch, padding_value=PAD_IDX, batch_first=False)\n",
        "    segment_labels_final = pad_sequence(segment_labels_batch, padding_value=PAD_IDX, batch_first=False)\n",
        "    is_nexts_batch = torch.tensor(is_nexts_batch, dtype=torch.long)\n",
        "\n",
        "    return bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_batch"
      ],
      "metadata": {
        "id": "wv4DIWK4rF1k"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BATCH_SIZE = 2\n",
        "\n",
        "# train_dataset_path = './bert_dataset/bert_train_data.csv'\n",
        "# test_dataset_path = './bert_dataset/bert_test_data.csv'\n",
        "\n",
        "# train_dataset = BERTCSVDataset(train_dataset_path)\n",
        "# test_dataset = BERTCSVDataset(test_dataset_path)\n",
        "\n",
        "# train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "# test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "## The DataLoader above is quite huge, and it will take several hours for the model to train with such a huge dataset.\n",
        "## Hence, below is the randomly sampled dataset (which is relatively small) from IMDB to make the process faster.\n",
        "\n",
        "BATCH_SIZE = 3\n",
        "\n",
        "train_dataset_path = './bert_dataset/bert_train_data_sampled.csv'\n",
        "test_dataset_path = './bert_dataset/bert_test_data_sampled.csv'\n",
        "\n",
        "train_dataset = BERTCSVDataset(train_dataset_path)\n",
        "test_dataset = BERTCSVDataset(test_dataset_path)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "891a52a452dc46978b771fcb8b688f12",
            "e3a0bc5470b4423aaffb751ebc8bc285",
            "e730c22867724429b639819681f79379",
            "86d6e16ec96e44158b4990629ec0241c",
            "48da45dad2bf4b54995aa48e47c4e512",
            "afdac4f7884f44e3ac1fd2b34b07b9f0",
            "44e1e50a99ad41fd9f0d4081f7ef9af9",
            "728a7ecf83ec47a88881c8a5c9456cfe",
            "93c643265ed24b04bf7629d936b19307",
            "d4e8c910d4a74a42a1ef959982da0db2",
            "583c9959ee1447ff9e8cb4b69c4ca55d",
            "69d1ce19d4e7434eb7d2cff4d42ed801",
            "8f351f631ba642e19f52c8a657e523f1",
            "be18207ce91c4ebeb3b729a42fca6a89",
            "ce1986e35711491daf1482c7f4cff0c9",
            "84d126440723468eaa0d54473ae49596",
            "0bad9977614d4707b7bd2e43454bc3a7",
            "3f270ab8f9a542fc91d8d9a3dd0bd6f3",
            "bec7e59f480d47e0b93c9fb1f9b75e53",
            "883291b042924acda5981e9040631b9b",
            "876c4835d5db4426bf770c192db95d2e",
            "1039138dc66546d39dbd6c2342446b16",
            "20ce7cd18c6c4ed6a68a122b0afb836a",
            "df034e4d592c479ca597b13c8a7b1cef",
            "bb81ae8a2fda48b599beb82f4c582aa0",
            "4aaf8fd6697e49609874c8d0d93cde19",
            "e19b840846ce4e21bd51927b1d4ab881",
            "a2c8e619ec67478585afb937d789a681",
            "19ee5d09ba34403e978c168b9bdfd611",
            "a1a199422d9d47d2a9234fbd0bee3fd8",
            "0ac95dbab2854a4aa5a234044e44ac61",
            "34de02c8d18842c597c674bbb10e0172",
            "673ef02f06484f1788b884b713fd7890",
            "255da14ec44b4a31b9e9fba4cdd724b3",
            "1063c31da5344930a42e3cbf319cab63",
            "dccb9edcd0424c9b976ecd82f3832f8c",
            "de77426b7c1b47a3aed11085caba6b21",
            "a161e79d4c334e65b8e0468562b48d27",
            "dc86e3d270e343da8ed05868ee4fabba",
            "4068e38d14a841e7a062915692296ee3",
            "c1d164ca95454444b3209cf11a598316",
            "38a3cd022537457ba367fdabbc3ffd92",
            "5325ee43c8634e5698d5b5c64314658c",
            "96ea3f79f1a6460181275b0ca7477a34"
          ]
        },
        "id": "3dPQTqxWsPvT",
        "outputId": "1e5110ef-da4b-49f2-c79a-490986c6000c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "891a52a452dc46978b771fcb8b688f12"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "69d1ce19d4e7434eb7d2cff4d42ed801"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "20ce7cd18c6c4ed6a68a122b0afb836a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "255da14ec44b4a31b9e9fba4cdd724b3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Model Creation**"
      ],
      "metadata": {
        "id": "GCcIAfT-sRTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "\n",
        "# Define the PositionalEncoding class as a PyTorch module for adding positional information to token embeddings\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        # Create a positional encoding matrix as per the Transformer paper's formula\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "    def forward(self, token_embedding: torch.Tensor):\n",
        "        # Apply the positional encodings to the input token embeddings\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "\n",
        "class BERTEmbedding (nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size ,dropout=0.1,train=True):\n",
        "        super().__init__()\n",
        "        self.token_embedding = TokenEmbedding( vocab_size,emb_size )\n",
        "        self.positional_encoding = PositionalEncoding(emb_size,dropout)\n",
        "        self.segment_embedding = nn.Embedding(3, emb_size)\n",
        "        self.dropout = torch.nn.Dropout(p=dropout)\n",
        "    def forward(self, bert_inputs, segment_labels=False):\n",
        "        my_embeddings = self.token_embedding(bert_inputs)\n",
        "        if self.train:\n",
        "            x = self.dropout(self.positional_encoding(my_embeddings) + self.segment_embedding(segment_labels))\n",
        "        else:\n",
        "            x = self.positional_encoding(my_embeddings)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "3kehIXMgsU55"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=768, n_layers=12, heads=12, dropout=0.1):\n",
        "        \"\"\"\n",
        "        vocab_size: The size of the vocabulary.\n",
        "        d_model: The size of the embeddings (hidden size).\n",
        "        n_layers: The number of Transformer layers.\n",
        "        heads: The number of attention heads in each Transformer layer.\n",
        "        dropout: The dropout rate applied to embeddings and Transformer layers.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_layers = n_layers\n",
        "        self.heads = heads\n",
        "\n",
        "        # Embedding layer that combines token embeddings and segment embeddings\n",
        "        self.bert_embedding = BERTEmbedding(vocab_size, d_model, dropout)\n",
        "        # Transformer Encoder layers\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=heads, dropout=dropout,batch_first=False)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=n_layers)\n",
        "        # Linear layer for Next Sentence Prediction\n",
        "        self.nextsentenceprediction = nn.Linear(d_model, 2)\n",
        "        # Linear layer for Masked Language Modeling\n",
        "        self.masked_language = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, bert_inputs, segment_labels):\n",
        "        \"\"\"\n",
        "        bert_inputs: Input tokens.\n",
        "        segment_labels: Segment IDs for distinguishing different segments in the input.\n",
        "        mask: Attention mask to prevent attention to padding tokens.\n",
        "\n",
        "        return: Predictions for next sentence task and masked language modeling task.\n",
        "        \"\"\"\n",
        "        padding_mask = (bert_inputs == PAD_IDX).transpose(0, 1)\n",
        "        # Generate embeddings from input tokens and segment labels\n",
        "        my_bert_embedding = self.bert_embedding(bert_inputs, segment_labels)\n",
        "        # Pass embeddings through the Transformer encoder\n",
        "        transformer_encoder_output = self.transformer_encoder(my_bert_embedding,src_key_padding_mask=padding_mask)\n",
        "        next_sentence_prediction = self.nextsentenceprediction(transformer_encoder_output[ 0,:])\n",
        "        # Masked Language Modeling: Predict all tokens in the sequence\n",
        "        masked_language = self.masked_language(transformer_encoder_output)\n",
        "        return  next_sentence_prediction, masked_language"
      ],
      "metadata": {
        "id": "g7qYMiG_s0H8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 147161\n",
        "EMBEDDING_DIM = 10\n",
        "d_model = EMBEDDING_DIM\n",
        "n_layers = 2\n",
        "initial_heads = 12\n",
        "initial_heads = 2\n",
        "heads = initial_heads - d_model % initial_heads\n",
        "\n",
        "dropout = 0.1  # Dropout rate\n",
        "\n",
        "# Create an instance of the BERT model\n",
        "model = BERT(vocab_size, d_model, n_layers, heads, dropout)"
      ],
      "metadata": {
        "id": "vBrF3glcwm6z"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Evaluation Function**"
      ],
      "metadata": {
        "id": "q4tuY5tI8BQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_IDX=0\n",
        "loss_fn_mlm = nn.CrossEntropyLoss(ignore_index=PAD_IDX)# The loss function must ignore PAD tokens and only calculates loss for the masked tokens\n",
        "loss_fn_nsp = nn.CrossEntropyLoss()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fu2CMdKw8Dv_",
        "outputId": "98d2abfc-ba3d-4982-a532-a0719d02045f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(dataloader=test_dataloader, model=model, loss_fn_mlm=loss_fn_mlm, loss_fn_nsp=loss_fn_nsp, device=device):\n",
        "    model.eval()  # Turn off dropout and other training-specific behaviors\n",
        "\n",
        "    total_loss = 0\n",
        "    total_next_sentence_loss = 0\n",
        "    total_mask_loss = 0\n",
        "    total_batches = 0\n",
        "    with torch.no_grad():  # Turn off gradients for validation, saves memory and computations\n",
        "        for batch in dataloader:\n",
        "            bert_inputs, bert_labels, segment_labels, is_nexts = [b.to(device) for b in batch]\n",
        "\n",
        "            # Forward pass\n",
        "            next_sentence_prediction, masked_language = model(bert_inputs, segment_labels)\n",
        "\n",
        "            # Calculate loss for next sentence prediction\n",
        "            # Ensure is_nexts is of the correct shape for CrossEntropyLoss\n",
        "            next_loss = loss_fn_nsp(next_sentence_prediction, is_nexts.view(-1))\n",
        "\n",
        "            # Calculate loss for predicting masked tokens\n",
        "            # Flatten both masked_language predictions and bert_labels to match CrossEntropyLoss input requirements\n",
        "            mask_loss = loss_fn_mlm(masked_language.view(-1, masked_language.size(-1)), bert_labels.view(-1))\n",
        "\n",
        "            # Sum up the two losses\n",
        "            loss = next_loss + mask_loss\n",
        "            if torch.isnan(loss):\n",
        "                continue\n",
        "            else:\n",
        "                total_loss += loss.item()\n",
        "                total_next_sentence_loss += next_loss.item()\n",
        "                total_mask_loss += mask_loss.item()\n",
        "                total_batches += 1\n",
        "\n",
        "    avg_loss = total_loss / (total_batches + 1)\n",
        "    avg_next_sentence_loss = total_next_sentence_loss / (total_batches + 1)\n",
        "    avg_mask_loss = total_mask_loss / (total_batches + 1)\n",
        "\n",
        "    print(f\"Average Loss: {avg_loss:.4f}, Average Next Sentence Loss: {avg_next_sentence_loss:.4f}, Average Mask Loss: {avg_mask_loss:.4f}\")\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "ckLt9ufQ8Ynq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Training**"
      ],
      "metadata": {
        "id": "UYoOl69P8sU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the optimizer\n",
        "optimizer = Adam(model.parameters(), lr=1e-4, weight_decay=0.01, betas=(0.9, 0.999))\n",
        "\n",
        "# Training loop setup\n",
        "num_epochs = 5\n",
        "total_steps = num_epochs * len(train_dataloader)\n",
        "\n",
        "# Define the number of warmup steps, e.g., 10% of total\n",
        "warmup_steps = int(total_steps * 0.1)\n",
        "\n",
        "# Create the learning rate scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=warmup_steps,\n",
        "                                            num_training_steps=total_steps)\n",
        "\n",
        "# Lists to store losses for plotting\n",
        "train_losses = []\n",
        "eval_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for step, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\")):\n",
        "        bert_inputs, bert_labels, segment_labels, is_nexts = [b.to(device) for b in batch]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        next_sentence_prediction, masked_language = model(bert_inputs, segment_labels)\n",
        "\n",
        "        next_loss = loss_fn_nsp(next_sentence_prediction, is_nexts)\n",
        "        mask_loss = loss_fn_mlm(masked_language.view(-1, masked_language.size(-1)), bert_labels.view(-1))\n",
        "\n",
        "        loss = next_loss + mask_loss\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update the learning rate\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if torch.isnan(loss):\n",
        "            continue\n",
        "        else:\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader) + 1\n",
        "    train_losses.append(avg_train_loss)\n",
        "    print(f\"Epoch {epoch+1} - Average training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Evaluation after each epoch\n",
        "    eval_loss = evaluate(test_dataloader, model, loss_fn_nsp, loss_fn_mlm, device)\n",
        "    eval_losses.append(eval_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQKKJ51I8woa",
        "outputId": "9df69387-68f2-43c9-84a8-681296e0d89d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 3334/3334 [01:46<00:00, 31.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Average training loss: 25.7529\n",
            "Average Loss: 12.3215, Average Next Sentence Loss: 0.6074, Average Mask Loss: 11.7141\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 3334/3334 [01:43<00:00, 32.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Average training loss: 24.0157\n",
            "Average Loss: 12.4446, Average Next Sentence Loss: 0.7045, Average Mask Loss: 11.7401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 3334/3334 [01:43<00:00, 32.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - Average training loss: 22.6847\n",
            "Average Loss: 12.4571, Average Next Sentence Loss: 0.6823, Average Mask Loss: 11.7747\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 3334/3334 [01:43<00:00, 32.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 - Average training loss: 22.1186\n",
            "Average Loss: 12.4500, Average Next Sentence Loss: 0.6568, Average Mask Loss: 11.7931\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 3334/3334 [01:43<00:00, 32.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 - Average training loss: 21.9311\n",
            "Average Loss: 12.4761, Average Next Sentence Loss: 0.6762, Average Mask Loss: 11.7999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6, 4))\n",
        "plt.scatter(range(1,num_epochs+1), train_losses, label=\"Training Loss\", color='blue')\n",
        "plt.scatter(range(1,num_epochs+1), eval_losses, label=\"Evaluation Loss\", color='orange')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Evaluation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "aCJ0HfZE9XZ8",
        "outputId": "dd3416fb-8c29-4cee-dddc-53251d39bd9d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Inference**"
      ],
      "metadata": {
        "id": "HjulWJyC9Z-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = BERT(vocab_size, d_model, n_layers, heads, dropout)  # Ensure these parameters match the original model's\n",
        "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/H04Cs7O75aOfmJ4YP2HdPw.pt'\n",
        "model1.load_state_dict(torch.load('H04Cs7O75aOfmJ4YP2HdPw.pt',map_location=torch.device('cpu')))\n",
        "model1.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHjkaxDc9kQL",
        "outputId": "6de2a584-0cac-4a1a-b474-b1158c475aee"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-06 15:55:35--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/H04Cs7O75aOfmJ4YP2HdPw.pt\n",
            "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 198.23.119.245\n",
            "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|198.23.119.245|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13099721 (12M) [binary/octet-stream]\n",
            "Saving to: ‘H04Cs7O75aOfmJ4YP2HdPw.pt’\n",
            "\n",
            "H04Cs7O75aOfmJ4YP2H 100%[===================>]  12.49M  24.5MB/s    in 0.5s    \n",
            "\n",
            "2025-07-06 15:55:36 (24.5 MB/s) - ‘H04Cs7O75aOfmJ4YP2HdPw.pt’ saved [13099721/13099721]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERT(\n",
              "  (bert_embedding): BERTEmbedding(\n",
              "    (token_embedding): TokenEmbedding(\n",
              "      (embedding): Embedding(147161, 10)\n",
              "    )\n",
              "    (positional_encoding): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (segment_embedding): Embedding(3, 10)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder_layer): TransformerEncoderLayer(\n",
              "    (self_attn): MultiheadAttention(\n",
              "      (out_proj): NonDynamicallyQuantizableLinear(in_features=10, out_features=10, bias=True)\n",
              "    )\n",
              "    (linear1): Linear(in_features=10, out_features=2048, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (linear2): Linear(in_features=2048, out_features=10, bias=True)\n",
              "    (norm1): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
              "    (norm2): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout1): Dropout(p=0.1, inplace=False)\n",
              "    (dropout2): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-1): 2 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=10, out_features=10, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=10, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=10, bias=True)\n",
              "        (norm1): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (nextsentenceprediction): Linear(in_features=10, out_features=2, bias=True)\n",
              "  (masked_language): Linear(in_features=10, out_features=147161, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_nsp(sentence1, sentence2, model, tokenizer):\n",
        "    # Tokenize sentences with special tokens\n",
        "    tokens = tokenizer.encode_plus(sentence1, sentence2, return_tensors=\"pt\")\n",
        "    tokens_tensor = tokens[\"input_ids\"].to(device)\n",
        "    segment_tensor = tokens[\"token_type_ids\"].to(device)\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        # Assuming the model returns NSP predictions first\n",
        "        nsp_prediction, _ = model(tokens_tensor, segment_tensor)\n",
        "        # Select the first element (first sequence) of the logits tensor\n",
        "        first_logits = nsp_prediction[0].unsqueeze(0)  # Adds an extra dimension, making it [1, 2]\n",
        "        logits = torch.softmax(first_logits, dim=1)\n",
        "        prediction = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    # Interpret the prediction\n",
        "    return \"Second sentence follows the first\" if prediction == 1 else \"Second sentence does not follow the first\"\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model1.eval()\n",
        "\n",
        "sentence1 = \"The cat sat on the mat.\"\n",
        "sentence2 = \"It was a sunny day\"\n",
        "\n",
        "print(predict_nsp(sentence1, sentence2, model1, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLmT0Edm9pgb",
        "outputId": "91a526c7-1500-4f3d-9491-14f35e075bcb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Second sentence follows the first\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "print(predict_nsp(sentence1, sentence2, model, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWyH_k3292tK",
        "outputId": "623e9c46-09e9-4abc-b437-a0cf50ea4c6c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Second sentence follows the first\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_mlm(sentence, model, tokenizer):\n",
        "    device = next(model.parameters()).device\n",
        "    # Tokenize the input sentence and convert to token IDs, including special tokens\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    tokens_tensor = inputs.input_ids.to(device)\n",
        "\n",
        "    # Create dummy segment labels filled with zeros, assuming it's needed by your model\n",
        "    segment_labels = torch.zeros_like(tokens_tensor).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Forward pass through the model, now correctly handling the output tuple\n",
        "        output_tuple = model(tokens_tensor, segment_labels)\n",
        "\n",
        "        # Assuming the second element of the tuple contains the MLM logits\n",
        "        predictions = output_tuple[1]  # Adjusted based on your model's output\n",
        "\n",
        "        # Identify the position of the [MASK] token\n",
        "        mask_token_index = (tokens_tensor == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
        "\n",
        "        # Get the predicted index for the [MASK] token from the MLM logits\n",
        "        predicted_index = torch.argmax(predictions[0, mask_token_index.item(), :], dim=-1)\n",
        "        predicted_token = tokenizer.convert_ids_to_tokens([predicted_index.item()])[0]\n",
        "\n",
        "        # Replace [MASK] in the original sentence with the predicted token\n",
        "        predicted_sentence = sentence.replace(tokenizer.mask_token, predicted_token, 1)\n",
        "\n",
        "    return predicted_sentence\n",
        "\n",
        "model1.eval()\n",
        "sentence = \"The cat sat on the [MASK].\"\n",
        "print(predict_mlm(sentence, model1, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0e6Cpu5-AJD",
        "outputId": "c18372e1-265f-4cba-9436-7ec80139a423"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cat sat on the [unused8].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "print(predict_mlm(sentence, model, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iav6KgLO-Txl",
        "outputId": "dc51cb3c-83a5-449f-cc2a-2665ba1a319a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cat sat on the [unused4].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NSP and MLM using BertForPreTraining (`bert-base-uncased`)"
      ],
      "metadata": {
        "id": "g7jMOV4Q9j7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForPreTraining, BertTokenizer\n",
        "import torch\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForPreTraining.from_pretrained('bert-base-uncased')\n",
        "\n",
        "text_1 = \"The cat sat on the mat\"\n",
        "text_2 = \"It was a sunny day\"\n",
        "inputs = tokenizer(text_1, text_2, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs, next_sentence_label=torch.LongTensor([1]))\n",
        "    nsp_logits = outputs.seq_relationship_logits\n",
        "\n",
        "if torch.argmax(nsp_logits, dim=-1).item() == 0:\n",
        "    print(\"The model thinks these sentences are NOT consecutive.\")\n",
        "else:\n",
        "    print(\"The model thinks these sentences are consecutive.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "8c5bbb662c3242b0a9d9c6d8079277c1",
            "61e5fcda8ca9412aaf2d89778dd20aeb",
            "9d52f13a89b84c8a8f0c9827b4b9ae18",
            "dee88f529d694a3881eaca8c50f9b0d3",
            "f46a82e9518c4f7d83b9724504dea0eb",
            "cd66f19786fe44559e836c8ba70ab348",
            "51ac732760dd48809b92179d81c8025e",
            "3908f0a223774acfa65753f3874e2417",
            "bc3981d4b8e54feabc69455adcaac9b5",
            "b9d9460ca036431d84f0c1bc16df4671",
            "56e3a85f6d7b40dbab7624263a0918c5"
          ]
        },
        "id": "f08XX7bh_ehz",
        "outputId": "126fb1b6-29dd-4d08-d6ad-482b93879210"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c5bbb662c3242b0a9d9c6d8079277c1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model thinks these sentences are NOT consecutive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "masked_text = \"The capital of France is [MASK].\"\n",
        "input_ids = tokenizer(masked_text, return_tensors=\"pt\")[\"input_ids\"]\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids=input_ids)\n",
        "    predictions = outputs.prediction_logits\n",
        "\n",
        "predicted_index = torch.argmax(predictions[0, input_ids[0] == tokenizer.mask_token_id]).item()\n",
        "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])\n",
        "\n",
        "print(f\"Predicted token: {predicted_token}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4Mkgl3f_jUT",
        "outputId": "cb4bf32a-07a2-4c29-d6a4-86533b272ed0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted token: ['paris']\n"
          ]
        }
      ]
    }
  ]
}