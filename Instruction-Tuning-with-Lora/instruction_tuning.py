# -*- coding: utf-8 -*-
"""Instruction-Tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kE69MgVV4tqScHY1RMrfdmkP-rI0vHD5

# Instruction-Tuning with LLMs
"""

# !pip install -Uq datasets transformers peft trl evaluate tqdm sacrebleu seaborn pandas matplotlib
# !pip install -Uq torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
# !pip install --upgrade evaluate
# !pip install trl==0.9.6

import warnings
warnings.filterwarnings('ignore')
warnings.simplefilter('ignore')

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from datasets import load_dataset
import torch
from torch.utils.data import Dataset
from tqdm import tqdm
import evaluate
from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM

from peft import get_peft_model, LoraConfig, TaskType

import pickle
import json
import matplotlib.pyplot as plt

from urllib.request import urlopen
import io

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

"""## Dataset

CodeAlpaca 20k dataset:
"""

!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WzOT_CwDALWedTtXjwH7bA/CodeAlpaca-20k.json

dataset = load_dataset("json", data_files="CodeAlpaca-20k.json", split="train")
dataset

dataset[1000]

""" Just focus on the examples that do not have any input:"""

dataset = dataset.filter(lambda example: example["input"] == '')
# Prevents the model from seeing examples in the same fixed order each time.
dataset = dataset.shuffle(seed=42)

dataset_split = dataset.train_test_split(test_size=0.2, seed=42)
train_dataset = dataset_split['train']
test_dataset = dataset_split['test']
dataset_split

# Select a small set of data for the resource limitation
tiny_test_dataset=test_dataset.select(range(10))
tiny_train_dataset=train_dataset.select(range(10))

"""## Model and tokenizer"""

model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m").to(device)
tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m", padding_side='left')

tokenizer.eos_token

"""## Preprocessing the data"""

def formatting_prompts_func(mydataset):
    output_texts = []
    for i in range(len(mydataset['instruction'])):
        text = (f"### Instruction:\n{mydataset['instruction'][i]}"
                f"\n\n### Response:\n{mydataset['output'][i]}</s>")
        output_texts.append(text)
    return output_texts


def formatting_prompts_func_no_response(mydataset):
    output_texts = []
    for i in range(len(mydataset['instruction'])):
        text = (f"### Instruction:\n{mydataset['instruction'][i]}"
                f"\n\n### Response:\n")
        output_texts.append(text)
    return output_texts

expected_outputs = []
instructions_with_responses = formatting_prompts_func(test_dataset)
instructions = formatting_prompts_func_no_response(test_dataset)

for i in tqdm(range(len(instructions_with_responses))):
    tokenized_instruction_with_response = tokenizer(instructions_with_responses[i], return_tensors="pt", max_length=1024, truncation=True, padding=False)
    tokenized_instruction = tokenizer(instructions[i], return_tensors="pt")
    expected_output = tokenizer.decode(tokenized_instruction_with_response['input_ids'][0][len(tokenized_instruction['input_ids'][0])-1:], skip_special_tokens=True)
    expected_outputs.append(expected_output)

print('Instructions:\n' + instructions[0] + "-------")
print('Instructions_with_responses:\n' + instructions_with_responses[0]+"-------")
print('Expected_outputs' + expected_outputs[0]+"-------")

"""convert the instructions list into a torch `Dataset`:"""

class ListDataset(Dataset):
    def __init__(self, original_list):
        self.original_list = original_list

    def __len__(self):
        return len(self.original_list)

    def __getitem__(self, i):
        return self.original_list[i]

instructions_torch = ListDataset(instructions)

"""## Test the base model"""

gen_pipeline = pipeline("text-generation",
                        model=model,
                        tokenizer=tokenizer,
                        device=device,
                        batch_size=2,
                        max_length=50,
                        truncation=True,
                        padding=False,
                        return_full_text=False)

"""Due to resource limitation, only apply the function on 3 records using `instructions_torch[:10]` and `max_length=50`:

"""

# leverages the pre-defined generation pipeline to generate outputs using the model
tokenizer.padding_side = 'left'

with torch.no_grad():
  # len(instructions_torch) = 1953
    pipeline_iterator= gen_pipeline(instructions_torch[:3], max_length=50, early_stopping=True)

generated_outputs_base = []
for text in pipeline_iterator:
    generated_outputs_base.append(text[0]["generated_text"])

"""Generated responses for the whole dataset:"""

urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/VvQRrSqS1P0_GobqtL-SKA/instruction-tuning-generated-outputs-base.pkl')
generated_outputs_base = pickle.load(io.BytesIO(urlopened.read()))

for i in range(1):
    print('Instruction '+ str(i+1) +': ')
    print(instructions[i])
    print('------\n')
    print('Expected response '+ str(i+1) +': ')
    print(expected_outputs[i])
    print('------\n')
    print('Generated response '+ str(i+1) +': ')
    print(generated_outputs_base[i])
    print('------\n')

"""## SacreBLEU score"""

sacrebleu = evaluate.load("sacrebleu")
results_base = sacrebleu.compute(predictions=generated_outputs_base, references=expected_outputs)
print(list(results_base.keys()))
print(round(results_base["score"], 1))

"""# Perform instruction fine-tuning with LoRA"""

lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=["q_proj", "v_proj"],
                         lora_dropout=0.1, task_type=TaskType.CAUSAL_LM)

model = get_peft_model(model, lora_config)

response_template = "### Response:\n"
collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)

training_args = SFTConfig(output_dir="/tmp", num_train_epochs=10, save_strategy="epoch",
                          fp16=True, per_device_train_batch_size=2, per_device_eval_batch_size=2,
                          max_seq_length=1024, do_eval=True)

trainer = SFTTrainer(model, train_dataset=train_dataset, eval_dataset=test_dataset,
                     formatting_func=formatting_prompts_func, args=training_args,
                     packing=False, data_collator=collator)

#trainer.train()
#log_history_lora = trainer.state.log_history

#trainer.save_model("./instruction_tuning_final_model_lora")

"""A model that was instruction fine-tuned to the above specifications on a GPU:"""

urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/49I70jQD0-RNRg2v-eOoxg/instruction-tuning-log-history-lora.json')
log_history_lora = json.load(io.BytesIO(urlopened.read()))

train_loss = [log["loss"] for log in log_history_lora if "loss" in log]

plt.figure(figsize=(10, 5))
plt.plot(train_loss, label='Training Loss')
plt.xlabel('Steps')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.legend()
plt.show()

gen_pipeline = pipeline("text-generation",
                        model=model,
                        tokenizer=tokenizer,
                        device=device,
                        batch_size=2,
                        max_length=50,
                        truncation=True,
                        padding=False,
                        return_full_text=False)

with torch.no_grad():
    pipeline_iterator= gen_pipeline(instructions_torch[:3],
                                max_length=50,
                                num_beams=5,
                                early_stopping=True,)
generated_outputs_lora = []
for text in pipeline_iterator:
    generated_outputs_lora.append(text[0]["generated_text"])


# generated texts for the entire dataset from the fine-tuned LoRA model and run on GPU:
urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/o7uYxe15xvX4CN-6Lr10iA/instruction-tuning-generated-outputs-lora.pkl')
generated_outputs_lora = pickle.load(io.BytesIO(urlopened.read()))

for i in range(1):
    print('Instruction '+ str(i+1) +': ')
    print(instructions[i])
    print('------\n')
    print('Expected response '+ str(i+1) +': ')
    print(expected_outputs[i])
    print('------\n')
    print('Generated response '+ str(i+1) +': ')
    print(generated_outputs_lora[i])
    print('------\n')

sacrebleu = evaluate.load("sacrebleu")
results_lora = sacrebleu.compute(predictions=generated_outputs_lora,
                                 references=expected_outputs)
print(list(results_lora.keys()))
print(round(results_lora["score"], 1))

"""### Try with another response template (Question-Answering)"""

def formatting_prompts_response_template(mydataset):
    output_texts = []
    for i in range(len(mydataset['instruction'])):
        text = (f"### Question:\n{mydataset['instruction'][i]}"
                f"\n\n### Answer:\n{mydataset['output'][i]}</s>")
        output_texts.append(text)
    return output_texts

def formatting_prompts_response_template_no_response(mydataset):
    output_texts = []
    for i in range(len(mydataset['instruction'])):
        text = (f"### Question:\n{mydataset['instruction'][i]}"
                f"\n\n### Answer:\n")
        output_texts.append(text)
    return output_texts

model_name = "EleutherAI/gpt-neo-125m"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

lora_config = LoraConfig(r=8, lora_alpha=16, target_modules=["q_proj", "v_proj"],
                         lora_dropout=0.1,
                         task_type=TaskType.CAUSAL_LM)

model = get_peft_model(model, lora_config)